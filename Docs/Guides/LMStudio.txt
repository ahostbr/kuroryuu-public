FULL OVERVIEW AT https://lmstudio.ai/docs/


Quick ping
This call will list the models that are loaded and ready.

curl http://0.0.0.0:1234/v1/models/
It is also a useful way to verify the server is reachable.

If you're on Windows and using Powershell, you can run this instead:

Invoke-RestMethod -Uri "http://0.0.0.0:1234/v1/models/"
This PowerShell command performs the same function as the curl command, sending a GET request to the specified endpoint and returning the list of available models.




Chat Completions
This is an OpenAI-like call to the /v1/chat/completion endpoint using the curl utility.
To run it on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://0.0.0.0:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/devstral-small-2-2512",
    "messages": [ 
      { "role": "system", "content": "Always answer in rhymes." },
      { "role": "user", "content": "Introduce yourself." }
    ], 
    "temperature": 0.7, 
    "max_tokens": -1,
    "stream": true
  }'
The call is 'stateless', which means the server does not retain the conversation history. It is the caller's responsibility to provide the whole conversation history in every call.

Streaming vs. accumulating a full response
Notice the "stream": true parameter. When set to true, LM Studio will stream back tokens as they are predicted.

If this parameter is set to false, the full prediction will be accumulated before the call returns. For longer generations or slower models this might take a while!


Structured Output
The API supports structured JSON outputs through the /v1/chat/completions endpoint when given a JSON schema. Doing this will cause the LLM to respond in valid JSON conforming to the schema provided.

It follows the same format as OpenAI's recently announced Structured Output API and is expected to work via the OpenAI client SDKs.

Example using curl
This example demonstrates a structured output request using the curl utility.

To run this example on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://0.0.0.0:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/devstral-small-2-2512",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful jokester."
      },
      {
        "role": "user",
        "content": "Tell me a joke."
      }
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "joke_response",
        "strict": "true",
        "schema": {
          "type": "object",
          "properties": {
            "joke": {
              "type": "string"
            }
          },
        "required": ["joke"]
        }
      }
    },
    "temperature": 0.7,
    "max_tokens": 50,
    "stream": false
  }'
All parameters recognized by /v1/chat/completions will be honored, and the JSON schema should be provided in the json_schema field of response_format.

The JSON object will be provided in string form in the typical response field, choices[0].message.content, and will need to be parsed into a JSON object.

Important: Not all models are capable of structured output, particularly LLMs below 7B parameters.

Check the model card if you are unsure if the model supports structured output.


What really is "tool use"?
Tool use describes:

LLMs output text requesting functions to be called (LLMs cannot directly execute code)
Your code executes those functions
Your code feeds the results back to the LLM.
Important
The model selected for tool use will greatly impact performance.

Some general guidance when selecting a model:

Not all models are capable of intelligent tool use
Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)
We've observed Qwen2.5-7B-Instruct to perform well in a wide variety of cases
This guidance may change
Example using Python
Below is a Python chatbot example that uses tools through OpenAI-like calls to the chat.completions API, enabling an LM Studio model to query wikipedia.

To run:

Pre-requisites:

Install Python on your machine (guide)
Install the OpenAI Python package (used only as a wrapper for LM Studio) with pip install openai
Steps:

Load a model
Start the server
Save the below example code with the "Save as Python File" button
Copy and paste the post-save command into your terminal and press Enter
tool-use-example.py

Save as Python file for a copy-and-paste command
Show Full Code (267 lines)
"""
LM Studio Tool Use Demo: Wikipedia Querying Chatbot
Demonstrates how an LM Studio model can query Wikipedia
"""

# Standard library imports
import itertools
import json
import shutil
import sys
import threading
import time
import urllib.parse
import urllib.request

# Third-party imports
from openai import OpenAI

# Initialize LM Studio client
client = OpenAI(base_url="http://0.0.0.0:1234/v1", api_key="lm-studio")
MODEL = "mistralai/devstral-small-2-2512"


def fetch_wikipedia_content(search_query: str) -> dict:
    """Fetches wikipedia content for a given search_query"""
    try:
        # Search for most relevant article
        search_url = "https://en.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "format": "json",
            "list": "search",
            "srsearch": search_query,
            "srlimit": 1,
        }

        url = f"{search_url}?{urllib.parse.urlencode(search_params)}"
        with urllib.request.urlopen(url) as response:
            search_data = json.loads(response.read().decode())

        if not search_data["query"]["search"]:
            return {
                "status": "error",
                "message": f"No Wikipedia article found for '{search_query}'",
            }

        # Get the normalized title from search results
        normalized_title = search_data["query"]["search"][0]["title"]

        # Now fetch the actual content with the normalized title
        content_params = {
            "action": "query",
            "format": "json",
            "titles": normalized_title,
            "prop": "extracts",
            "exintro": "true",
            "explaintext": "true",
            "redirects": 1,
        }

        url = f"{search_url}?{urllib.parse.urlencode(content_params)}"
        with urllib.request.urlopen(url) as response:
            data = json.loads(response.read().decode())

        pages = data["query"]["pages"]
        page_id = list(pages.keys())[0]

        if page_id == "-1":
            return {
                "status": "error",
                "message": f"No Wikipedia article found for '{search_query}'",
            }

        content = pages[page_id]["extract"].strip()
        return {
            "status": "success",
            "content": content,
            "title": pages[page_id]["title"],
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}


# Define tool for LM Studio
WIKI_TOOL = {
    "type": "function",
    "function": {
        "name": "fetch_wikipedia_content",
        "description": (
            "Search Wikipedia and fetch the introduction of the most relevant article. "
            "Always use this if the user is asking for something that is likely on wikipedia. "
            "If the user has a typo in their search query, correct it before searching."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "search_query": {
                    "type": "string",
                    "description": "Search query for finding the Wikipedia article",
                },
            },
            "required": ["search_query"],
        },
    },
}


# Class for displaying the state of model processing
class Spinner:
    def __init__(self, message="Processing..."):
        self.spinner = itertools.cycle(["-", "/", "|", "\\"])
        self.busy = False
        self.delay = 0.1
        self.message = message
        self.thread = None

    def write(self, text):
        sys.stdout.write(text)
        sys.stdout.flush()

    def _spin(self):
        while self.busy:
            self.write(f"\r{self.message} {next(self.spinner)}")
            time.sleep(self.delay)
        self.write("\r\033[K")  # Clear the line

    def __enter__(self):
        self.busy = True
        self.thread = threading.Thread(target=self._spin)
        self.thread.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.busy = False
        time.sleep(self.delay)
        if self.thread:
            self.thread.join()
        self.write("\r")  # Move cursor to beginning of line


def chat_loop():
    """
    Main chat loop that processes user input and handles tool calls.
    """
    messages = [
        {
            "role": "system",
            "content": (
                "You are an assistant that can retrieve Wikipedia articles. "
                "When asked about a topic, you can retrieve Wikipedia articles "
                "and cite information from them."
            ),
        }
    ]

    print(
        "Assistant: "
        "Hi! I can access Wikipedia to help answer your questions about history, "
        "science, people, places, or concepts - or we can just chat about "
        "anything else!"
    )
    print("(Type 'quit' to exit)")

    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() == "quit":
            break

        messages.append({"role": "user", "content": user_input})
        try:
            with Spinner("Thinking..."):
                response = client.chat.completions.create(
                    model=MODEL,
                    messages=messages,
                    tools=[WIKI_TOOL],
                )

            if response.choices[0].message.tool_calls:
                # Handle all tool calls
                tool_calls = response.choices[0].message.tool_calls

                # Add all tool calls to messages
                messages.append(
                    {
                        "role": "assistant",
                        "tool_calls": [
                            {
                                "id": tool_call.id,
                                "type": tool_call.type,
                                "function": tool_call.function,
                            }
                            for tool_call in tool_calls
                        ],
                    }
                )

                # Process each tool call and add results
                for tool_call in tool_calls:
                    args = json.loads(tool_call.function.arguments)
                    result = fetch_wikipedia_content(args["search_query"])

                    # Print the Wikipedia content in a formatted way
                    terminal_width = shutil.get_terminal_size().columns
                    print("\n" + "=" * terminal_width)
                    if result["status"] == "success":
                        print(f"\nWikipedia article: {result['title']}")
                        print("-" * terminal_width)
                        print(result["content"])
                    else:
                        print(
                            f"\nError fetching Wikipedia content: {result['message']}"
                        )
                    print("=" * terminal_width + "\n")

                    messages.append(
                        {
                            "role": "tool",
                            "content": json.dumps(result),
                            "tool_call_id": tool_call.id,
                        }
                    )

                # Stream the post-tool-call response
                print("\nAssistant:", end=" ", flush=True)
                stream_response = client.chat.completions.create(
                    model=MODEL, messages=messages, stream=True
                )
                collected_content = ""
                for chunk in stream_response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        print(content, end="", flush=True)
                        collected_content += content
                print()  # New line after streaming completes
                messages.append(
                    {
                        "role": "assistant",
                        "content": collected_content,
                    }
                )
            else:
                # Handle regular response
                print("\nAssistant:", response.choices[0].message.content)
                messages.append(
                    {
                        "role": "assistant",
                        "content": response.choices[0].message.content,
                    }
                )

        except Exception as e:
            print(
                f"\nError chatting with the LM Studio server!\n\n"
                f"Please ensure:\n"
                f"1. LM Studio server is running at 0.0.0.0:1234 (hostname:port)\n"
                f"2. Model '{MODEL}' is downloaded\n"
                f"3. Model '{MODEL}' is loaded, or that just-in-time model loading is enabled\n\n"
                f"Error details: {str(e)}\n"
                "See https://lmstudio.ai/docs/basics/server for more information"
            )
            exit(1)


if __name__ == "__main__":
    chat_loop()

Start the server, then run the above script to chat with a model that is capable of querying wikipedia:

-> % python tool-use-example.py

Assistant: Hi! I can access Wikipedia to help answer your questions about history, science, people, places, or concepts - or we can just chat about anything else!
(Type 'quit' to exit)

You:
Example using curl
This example demonstrates a model requesting a tool call using the curl utility.

To run this example on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://0.0.0.0:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/devstral-small-2-2512",
    "messages": [{"role": "user", "content": "What dell products do you have under $50 in electronics?"}],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "search_products",
          "description": "Search the product catalog by various criteria. Use this whenever a customer asks about product availability, pricing, or specifications.",
          "parameters": {
            "type": "object",
            "properties": {
              "query": {
                "type": "string",
                "description": "Search terms or product name"
              },
              "category": {
                "type": "string", 
                "description": "Product category to filter by",
                "enum": ["electronics", "clothing", "home", "outdoor"]
              },
              "max_price": {
                "type": "number",
                "description": "Maximum price in dollars"
              }
            },
            "required": ["query"],
            "additionalProperties": false
          }
        }
      }
    ]
  }'
All parameters recognized by /v1/chat/completions will be honored, and the array of available tools should be provided in the tools field.

If the model decides that the user message would be best fulfilled with a tool call, an array of tool call request objects will be provided in the response field, choices[0].message.tool_calls.

The finish_reason field of the top-level response object will also be populated with "tool_calls".

An example response to the above curl request will look like:

{
  "id": "chatcmpl-gb1t1uqzefudice8ntxd9i",
  "object": "chat.completion",
  "created": 1730913210,
  "model": "mistralai/devstral-small-2-2512",
  "choices": [
    {
      "index": 0,
      "logprobs": null,
      "finish_reason": "tool_calls",
      "message": {
        "role": "assistant",
        "tool_calls": [
          {
            "id": "365174485",
            "type": "function",
            "function": {
              "name": "search_products",
              "arguments": "{\"query\":\"dell\",\"category\":\"electronics\",\"max_price\":50}"
            }
          }
        ]
      }
    }
  ],
  "usage": {
    "prompt_tokens": 263,
    "completion_tokens": 34,
    "total_tokens": 297
  },
  "system_fingerprint": "mistralai/devstral-small-2-2512"
}
In plain english, the above response can be thought of as the model saying:

"Please call the search_products function, with arguments:

'dell' for the query parameter,
'electronics' for the category parameter
'50' for the max_price parameter
and give me back the results"

The tool_calls field will need to be parsed to call actual functions/APIs, as shown in the Python example code.

High-level flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SETUP: LLM + Tool list   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Get user input        â”‚â—„â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
           â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ LLM prompted w/messages  â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
           â–¼                     â”‚
     Needs tools?                â”‚
      â”‚         â”‚                â”‚
    Yes         No               â”‚
      â”‚         â”‚                â”‚
      â–¼         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚Tool Responseâ”‚              â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
       â–¼                     â”‚   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚Execute toolsâ”‚              â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
       â–¼                     â–¼   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Add results  â”‚          â”‚  Normal   â”‚
â”‚to messages  â”‚          â”‚ response  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                       â–²
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Tool List and Functions
It is critical to:

Write functions for the model to use
Provide a list of these functions and their parameters to the model through the tools field of the /v1/chat/completions request body
In Python, for example:

# Function definition
def search_products(query: str, category: str = None, max_price: float = None):
    """
    Search products in catalog based on criteria
    Args:
        query: Search terms
        category: Product category filter
        max_price: Maximum price filter
    Returns:
        Search results
    """
    # Implementation here
    pass

# Corresponding Tool List Definition
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_products",
            "description": "Search the product catalog by various criteria",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search terms or product name"
                    },
                    "category": {
                        "type": "string",
                        "description": "Product category to filter by",
                        "enum": ["electronics", "clothing", "home", "outdoor"]
                    },
                    "max_price": {
                        "type": "number",
                        "description": "Maximum price in dollars"
                    }
                },
                "required": ["query"]
            }
        }
    }
]

General Usage Flow:

Send tools list with API request
Parse model's tool_calls response
Execute matching function with provided arguments
Return results to model
Online Documentation
For more in-depth documentation, visit the online tool use docs.
https://lmstudio.ai/docs/advanced/tool-use

Image input
The API supports requests containing images when a vision-enabled model (e.g. LLaVA) is loaded. Images are passed in using the messages array in a request to the /v1/chat/completions endpoint.

Important: The API does not recognize filepaths, so images must be encoded in base64. See pro tip for this below.

Example using curl
This example demonstrates image input over the API using the curl utility. To run this example on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://0.0.0.0:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/devstral-small-2-2512",
    "messages": [
      { 
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What is this image?"
          },
          {
            "type": "image_url",
            "image_url": { "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABsAAAAbCAYAAACN1PRVAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAQ2SURBVEhL3ZVrTBxVFMf/d2f2vcsCWxZYZKVQW4vVUmmtNpgaSSA1NUWpkFhpWkOwxsZa0WpqjMYXaTQR4wdjSttQ2tDaGIRqa5HWVmwFYgwasRhoeRipi1BWdpd9sDvjndmBZbzgt/ZDf1927jl3zv+ec8+ZJbcvLRBxk9AovzeFW1ds3jtzcQ6YHXcCQR+EsQEQuwvEaAPvGUCKdhK8cRq/BP1wTxKIyttjj78Iw9VuWLrPxQzzwIhtwb1wl+2FL8GGTcfq8U9SMlo3liK7vxdFJz+HzhBC/s52WF0e+ENAxxUeX/fwaPAWgBu/Bu3ffyiRWJgyuktfgc9KhY7XY8psUQnpzQGs3nVBFpIw64HC3AjefyKIrk1nsQxD4ECwf2yd7P8vjNi0XhcTMllw+rFyZA5dlYVMiT6s2X0BloxJZacal13EZzt82K5PRc60RbGqYcpYoy1BxJqM5rIKZAwPYkPzcSQ6xpH3zCUYkgPKrnkIcQi9W4zosA3b7ZfQyU0ojjhMZj5rAk5u3jIrZL9tlJbuu/8XktBGwT/UB1NtE5YUuBWjGkbsYnEJFrn/koUc2SNYVtmOs70RNHcQTHjVjRukDdLWDXzRQbvRB3AP9wGJQTy1NqzsUMOUseSR/ShuaYJz+SAySjvxXJ0G3ilApxURFQg+rooiKxWY9It49hMe417aKAYqTON/WBlFID0Hd4lXsOZtKyamiBI1BpPZhlPHkLmyH/c83YnuYRHjtB+OVkdx6HlB9v/QGwvw86AGI9eBw7sjOPwCFUldjELdAeQF67Ak3Ii0FVnyvrkwmTXlu7Cq7FfqiZXJQ7NKS4r5rtMy8vR4CWaCUFg6CIFzUcxX5K/B3TSjt8xHcSp6Hyp81UirqcDs1FOYzBxFPbKQhIHO0YyQRLKVyELSIU7/RGDUxwN5uQSsNfSjPPw6zCSIMG+GoKP1nQMjNuJhTAx/jgMftXDYWsvLGUq8xh9BRXgvOoXlGBZTYZoagyak7mAmcp87bvommo+sQKOyipPjBHY9KsAXIDjjX4HzQh4smgDOGPagmj+BPeEq2LpalN1xmDvbvDqMD8qC8vOokIQWYR0q+a/k9Qw9Q8DOT3mIJjPIS28o1hgcBDzg/x4N+9gPMiPGa0S0VvuQnaIyqxDppTd8q8GhNg6NL0dU9ypRVW9Ea49WWcVhyhihs/TOl/GLFX530KGinTKH34aJLHSHU0RKgvpQPw5yaLvMCklwiXbXm8rzLANjHDKTosh1CgjXrocm3QuSRqdXIcUmYlWOiCfXC9Dr4oM7FSbYWmei46Ie5hkWbL0DR9Jx8dUHIU6YoMm9plhjEEKwcrH05YgHPd/L4f59SejXuBQLy4JihUEnPKNGlCe342AXT7Nlt0p/nucu89jRYMS2g2aMLC2Ee9t7ipeFaZAbyYKZ3QhuVTHgX54yfufoA1ofAAAAAElFTkSuQmCC" }
          }
        ]
      }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
}'
The response will be received in the standard response format to the /v1/chat/completions endpoint.

Note that large images will fail here due to their base64 encodings surpassing curl's length limit. To process large images, please query the LLM using another method such as the TypeScript SDK.

Multiple images
Multiple images can be passed to the LLM by simply specifying more image-url objects in the content array.

Pro tip for converting images to base64
Suppose your local image exists in the path /path/to/image. You can use the base64 utility packaged with GNU coreutils like so:

  {
    "type": "image_url",
    "image_url": { "url": "data:image/png;base64,'"$( base64 -w 0 /path/to/image )"'" }
  }
â„¹ï¸ ğŸ‘¾ To learn more about LM Studio local server endpoints, visit the documentation.
https://lmstudio.ai/docs




Structured Output

Copy as Markdown
Enforce LLM response formats using JSON schemas.

You can enforce a particular response format from an LLM by providing a JSON schema to the /v1/chat/completions endpoint, via LM Studio's REST API (or via any OpenAI client).

Start LM Studio as a server
To use LM Studio programmatically from your own code, run LM Studio as a local server.

You can turn on the server from the "Developer" tab in LM Studio, or via the lms CLI:

lms server start

Install lms by running npx lmstudio install-cli
This will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see REST API Overview.

Structured Output
The API supports structured JSON outputs through the /v1/chat/completions endpoint when given a JSON schema. Doing this will cause the LLM to respond in valid JSON conforming to the schema provided.

It follows the same format as OpenAI's recently announced Structured Output API and is expected to work via the OpenAI client SDKs.

Example using curl

This example demonstrates a structured output request using the curl utility.

To run this example on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "{{model}}",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful jokester."
      },
      {
        "role": "user",
        "content": "Tell me a joke."
      }
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "joke_response",
        "strict": "true",
        "schema": {
          "type": "object",
          "properties": {
            "joke": {
              "type": "string"
            }
          },
          "required": ["joke"]
        }
      }
    },
    "temperature": 0.7,
    "max_tokens": 50,
    "stream": false
  }'

All parameters recognized by /v1/chat/completions will be honored, and the JSON schema should be provided in the json_schema field of response_format.

The JSON object will be provided in string form in the typical response field, choices[0].message.content, and will need to be parsed into a JSON object.

Example using python

from openai import OpenAI
import json

# Initialize OpenAI client that points to the local LM Studio server
client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"
)

# Define the conversation with the AI
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Create 1-3 fictional characters"}
]

# Define the expected response structure
character_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "characters",
        "schema": {
            "type": "object",
            "properties": {
                "characters": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "occupation": {"type": "string"},
                            "personality": {"type": "string"},
                            "background": {"type": "string"}
                        },
                        "required": ["name", "occupation", "personality", "background"]
                    },
                    "minItems": 1,
                }
            },
            "required": ["characters"]
        },
    }
}

# Get response from AI
response = client.chat.completions.create(
    model="your-model",
    messages=messages,
    response_format=character_schema,
)

# Parse and display the results
results = json.loads(response.choices[0].message.content)
print(json.dumps(results, indent=2))

Important: Not all models are capable of structured output, particularly LLMs below 7B parameters.

Check the model card README if you are unsure if the model supports structured output.

Structured output engine
For GGUF models: utilize llama.cpp's grammar-based sampling APIs.
For MLX models: using Outlines.
The MLX implementation is available on Github: lmstudio-ai/mlx-engine.


Tool Use

Copy as Markdown
Enable LLMs to interact with external functions and APIs.

Tool use enables LLMs to request calls to external functions and APIs through the /v1/chat/completions and v1/responses endpoints (Learn more), via LM Studio's REST API (or via any OpenAI client). This expands their functionality far beyond text output.

Quick Start
1. Start LM Studio as a server
To use LM Studio programmatically from your own code, run LM Studio as a local server.

You can turn on the server from the "Developer" tab in LM Studio, or via the lms CLI:

lms server start

Install lms by running npx lmstudio install-cli
This will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see REST API Overview.

2. Load a Model
You can load a model from the "Chat" or "Developer" tabs in LM Studio, or via the lms CLI:

lms load

3. Copy, Paste, and Run an Example!
Curl
Single Turn Tool Call Request
Python
Single Turn Tool Call + Tool Use
Multi-Turn Example
Advanced Agent Example
Tool Use
What really is "Tool Use"?
Tool use describes:

LLMs output text requesting functions to be called (LLMs cannot directly execute code)
Your code executes those functions
Your code feeds the results back to the LLM.
High-level flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SETUP: LLM + Tool list   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Get user input        â”‚â—„â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
           â–¼                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ LLM prompted w/messages  â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
           â–¼                     â”‚
     Needs tools?                â”‚
      â”‚         â”‚                â”‚
    Yes         No               â”‚
      â”‚         â”‚                â”‚
      â–¼         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚Tool Responseâ”‚              â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
       â–¼                     â”‚   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚Execute toolsâ”‚              â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
       â–¼                     â–¼   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Add results  â”‚          â”‚  Normal   â”‚
â”‚to messages  â”‚          â”‚ response  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                       â–²
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

In-depth flow
LM Studio supports tool use through the /v1/chat/completions endpoint when given function definitions in the tools parameter of the request body. Tools are specified as an array of function definitions that describe their parameters and usage, like:

It follows the same format as OpenAI's Function Calling API and is expected to work via the OpenAI client SDKs.

We will use lmstudio-community/Qwen2.5-7B-Instruct-GGUF as the model in this example flow.

You provide a list of tools to an LLM. These are the tools that the model can request calls to. For example:
// the list of tools is model-agnostic
[
  {
    "type": "function",
    "function": {
      "name": "get_delivery_date",
      "description": "Get the delivery date for a customer's order",
      "parameters": {
        "type": "object",
        "properties": {
          "order_id": {
            "type": "string"
          }
        },
        "required": ["order_id"]
      }
    }
  }
]

This list will be injected into the system prompt of the model depending on the model's chat template. For Qwen2.5-Instruct, this looks like:

<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.

# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"type": "function", "function": {"name": "get_delivery_date", "description": "Get the delivery date for a customer's order", "parameters": {"type": "object", "properties": {"order_id": {"type": "string"}}, "required": ["order_id"]}}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>

Important: The model can only request calls to these tools because LLMs cannot directly call functions, APIs, or any other tools. They can only output text, which can then be parsed to programmatically call the functions.

When prompted, the LLM can then decide to either:

(a) Call one or more tools
User: Get me the delivery date for order 123
Model: <tool_call>
{"name": "get_delivery_date", "arguments": {"order_id": "123"}}
</tool_call>

(b) Respond normally
User: Hi
Model: Hello! How can I assist you today?

LM Studio parses the text output from the model into an OpenAI-compliant chat.completion response object.

If the model was given access to tools, LM Studio will attempt to parse the tool calls into the response.choices[0].message.tool_calls field of the chat.completion response object.
If LM Studio cannot parse any correctly formatted tool calls, it will simply return the response to the standard response.choices[0].message.content field.
Note: Smaller models and models that were not trained for tool use may output improperly formatted tool calls, resulting in LM Studio being unable to parse them into the tool_calls field. This is useful for troubleshooting when you do not receive tool_calls as expected. Example of an improperly formatting Qwen2.5-Instruct tool call:
<tool_call>
["name": "get_delivery_date", function: "date"]
</tool_call>

Note that the brackets are incorrect, and the call does not follow the name, argument format.

Your code parses the chat.completion response to check for tool calls from the model, then calls the appropriate tools with the parameters specified by the model. Your code then adds both:

The model's tool call message
The result of the tool call
To the messages array to send back to the model

# pseudocode, see examples for copy-paste snippets
if response.has_tool_calls:
    for each tool_call:
        # Extract function name & args
        function_to_call = tool_call.name     # e.g. "get_delivery_date"
        args = tool_call.arguments            # e.g. {"order_id": "123"}

        # Execute the function
        result = execute_function(function_to_call, args)

        # Add result to conversation
        add_to_messages([
            ASSISTANT_TOOL_CALL_MESSAGE,      # The request to use the tool
            TOOL_RESULT_MESSAGE               # The tool's response
        ])
else:
    # Normal response without tools
    add_to_messages(response.content)

The LLM is then prompted again with the updated messages array, but without access to tools. This is because:

The LLM already has the tool results in the conversation history
We want the LLM to provide a final response to the user, not call more tools
# Example messages
messages = [
    {"role": "user", "content": "When will order 123 be delivered?"},
    {"role": "assistant", "function_call": {
        "name": "get_delivery_date",
        "arguments": {"order_id": "123"}
    }},
    {"role": "tool", "content": "2024-03-15"},
]
response = client.chat.completions.create(
    model="lmstudio-community/qwen2.5-7b-instruct",
    messages=messages
)

The response.choices[0].message.content field after this call may be something like:

Your order #123 will be delivered on March 15th, 2024

The loop continues back at step 2 of the flow

Note: This is the pedantic flow for tool use. However, you can certainly experiment with this flow to best fit your use case.

Supported Models
Through LM Studio, all models support at least some degree of tool use.

However, there are currently two levels of support that may impact the quality of the experience: Native and Default.

Models with Native tool use support will have a hammer badge in the app, and generally perform better in tool use scenarios.

Native tool use support
"Native" tool use support means that both:

The model has a chat template that supports tool use (usually means the model has been trained for tool use)
This is what will be used to format the tools array into the system prompt and tell them model how to format tool calls
Example: Qwen2.5-Instruct chat template
LM Studio supports that model's tool use format
Required for LM Studio to properly input the chat history into the chat template, and parse the tool calls the model outputs into the chat.completion object
Models that currently have native tool use support in LM Studio (subject to change):

Qwen
GGUF lmstudio-community/Qwen2.5-7B-Instruct-GGUF (4.68 GB)
MLX mlx-community/Qwen2.5-7B-Instruct-4bit (4.30 GB)
Llama-3.1, Llama-3.2
GGUF lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF (4.92 GB)
MLX mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (8.54 GB)
Mistral
GGUF bartowski/Ministral-8B-Instruct-2410-GGUF (4.67 GB)
MLX mlx-community/Ministral-8B-Instruct-2410-4bit (4.67 GB GB)
Default tool use support
"Default" tool use support means that either:

The model does not have chat template that supports tool use (usually means the model has not been trained for tool use)
LM Studio does not currently support that model's tool use format
Under the hood, default tool use works by:

Giving models a custom system prompt and a default tool call format to use
Converting tool role messages to the user role so that chat templates without the tool role are compatible
Converting assistant role tool_calls into the default tool call format
Results will vary by model.

You can see the default format by running lms log stream in your terminal, then sending a chat completion request with tools to a model that doesn't have Native tool use support. The default format is subject to change.

Expand to see example of default tool use format
All models that don't have native tool use support will have default tool use support.

Example using curl
This example demonstrates a model requesting a tool call using the curl utility.

To run this example on Mac or Linux, use any terminal. On Windows, use Git Bash.

curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "lmstudio-community/qwen2.5-7b-instruct",
    "messages": [{"role": "user", "content": "What dell products do you have under $50 in electronics?"}],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "search_products",
          "description": "Search the product catalog by various criteria. Use this whenever a customer asks about product availability, pricing, or specifications.",
          "parameters": {
            "type": "object",
            "properties": {
              "query": {
                "type": "string",
                "description": "Search terms or product name"
              },
              "category": {
                "type": "string",
                "description": "Product category to filter by",
                "enum": ["electronics", "clothing", "home", "outdoor"]
              },
              "max_price": {
                "type": "number",
                "description": "Maximum price in dollars"
              }
            },
            "required": ["query"],
            "additionalProperties": false
          }
        }
      }
    ]
  }'

All parameters recognized by /v1/chat/completions will be honored, and the array of available tools should be provided in the tools field.

If the model decides that the user message would be best fulfilled with a tool call, an array of tool call request objects will be provided in the response field, choices[0].message.tool_calls.

The finish_reason field of the top-level response object will also be populated with "tool_calls".

An example response to the above curl request will look like:

{
  "id": "chatcmpl-gb1t1uqzefudice8ntxd9i",
  "object": "chat.completion",
  "created": 1730913210,
  "model": "lmstudio-community/qwen2.5-7b-instruct",
  "choices": [
    {
      "index": 0,
      "logprobs": null,
      "finish_reason": "tool_calls",
      "message": {
        "role": "assistant",
        "tool_calls": [
          {
            "id": "365174485",
            "type": "function",
            "function": {
              "name": "search_products",
              "arguments": "{\"query\":\"dell\",\"category\":\"electronics\",\"max_price\":50}"
            }
          }
        ]
      }
    }
  ],
  "usage": {
    "prompt_tokens": 263,
    "completion_tokens": 34,
    "total_tokens": 297
  },
  "system_fingerprint": "lmstudio-community/qwen2.5-7b-instruct"
}

In plain english, the above response can be thought of as the model saying:

"Please call the search_products function, with arguments:

'dell' for the query parameter,
'electronics' for the category parameter
'50' for the max_price parameter
and give me back the results"

The tool_calls field will need to be parsed to call actual functions/APIs. The below examples demonstrate how.

Examples using python
Tool use shines when paired with program languages like python, where you can implement the functions specified in the tools field to programmatically call them when the model requests.

Single-turn example
Below is a simple single-turn (model is only called once) example of enabling a model to call a function called say_hello that prints a hello greeting to the console:

single-turn-example.py

from openai import OpenAI

# Connect to LM Studio
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

# Define a simple function
def say_hello(name: str) â†’ str:
    print(f"Hello, {name}!")

# Tell the AI about our function
tools = [
    {
        "type": "function",
        "function": {
            "name": "say_hello",
            "description": "Says hello to someone",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "The person's name"
                    }
                },
                "required": ["name"]
            }
        }
    }
]

# Ask the AI to use our function
response = client.chat.completions.create(
    model="lmstudio-community/qwen2.5-7b-instruct",
    messages=[{"role": "user", "content": "Can you say hello to Bob the Builder?"}],
    tools=tools
)

# Get the name the AI wants to use a tool to say hello to
# (Assumes the AI has requested a tool call and that tool call is say_hello)
tool_call = response.choices[0].message.tool_calls[0]
name = eval(tool_call.function.arguments)["name"]

# Actually call the say_hello function
say_hello(name) # Prints: Hello, Bob the Builder!


Running this script from the console should yield results like:

â†’ % python single-turn-example.py
Hello, Bob the Builder!

Play around with the name in

messages=[{"role": "user", "content": "Can you say hello to Bob the Builder?"}]

to see the model call the say_hello function with different names.

Multi-turn example
Now for a slightly more complex example.

In this example, we'll:

Enable the model to call a get_delivery_date function
Hand the result of calling that function back to the model, so that it can fulfill the user's request in plain text
multi-turn-example.py (click to expand)
Running this script from the console should yield results like:

â†’ % python multi-turn-example.py

Model response requesting tool call:

ChatCompletion(id='chatcmpl-wwpstqqu94go4hvclqnpwn', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='377278620', function=Function(arguments='{"order_id":"1017"}', name='get_delivery_date'), type='function')]))], created=1730916196, model='lmstudio-community/qwen2.5-7b-instruct', object='chat.completion', service_tier=None, system_fingerprint='lmstudio-community/qwen2.5-7b-instruct', usage=CompletionUsage(completion_tokens=24, prompt_tokens=223, total_tokens=247, completion_tokens_details=None, prompt_tokens_details=None))

get_delivery_date function returns delivery date:

2024-11-19 13:03:17.773298

Final model response with knowledge of the tool call result:

Your order number 1017 is scheduled for delivery on November 19, 2024, at 13:03 PM.

Advanced agent example
Building upon the principles above, we can combine LM Studio models with locally defined functions to create an "agent" - a system that pairs a language model with custom functions to understand requests and perform actions beyond basic text generation.

The agent in the below example can:

Open safe urls in your default browser
Check the current time
Analyze directories in your file system
agent-chat-example.py (click to expand)
Running this script from the console will allow you to chat with the agent:

â†’ % python agent-example.py
Assistant: Hello! I can help you open safe web links, tell you the current time, and analyze directory contents. What would you like me to do?
(Type 'quit' to exit)

You: What time is it?

Assistant: The current time is 14:11:40 (EST) as of November 6, 2024.

You: What time is it now?

Assistant: The current time is 14:13:59 (EST) as of November 6, 2024.

You: Open lmstudio.ai

Assistant: The link to lmstudio.ai has been opened in your default web browser.

You: What's in my current directory?

Assistant: Your current directory at `/Users/matt/project` contains a total of 14 files and 8 directories. Here's the breakdown:

- Files without an extension: 3
- `.mjs` files: 2
- `.ts` (TypeScript) files: 3
- Markdown (`md`) file: 1
- JSON files: 4
- TOML file: 1

The total size of these items is 1,566,990,604 bytes.

You: Thank you!

Assistant: You're welcome! If you have any other questions or need further assistance, feel free to ask.

You:

Streaming
When streaming through /v1/chat/completions (stream=true), tool calls are sent in chunks. Function names and arguments are sent in pieces via chunk.choices[0].delta.tool_calls.function.name and chunk.choices[0].delta.tool_calls.function.arguments.

For example, to call get_current_weather(location="San Francisco"), the streamed ChoiceDeltaToolCall in each chunk.choices[0].delta.tool_calls[0] object will look like:

ChoiceDeltaToolCall(index=0, id='814890118', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')
ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{"', name=None), type=None)
ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)
ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='":"', name=None), type=None)
ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San Francisco', name=None), type=None)
ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='"}', name=None), type=None)

These chunks must be accumulated throughout the stream to form the complete function signature for execution.

The below example shows how to create a simple tool-enhanced chatbot through the /v1/chat/completions streaming endpoint (stream=true).

tool-streaming-chatbot.py (click to expand)
You can chat with the bot by running this script from the console:

â†’ % python tool-streaming-chatbot.py
Assistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)

You: Tell me a joke, then tell me the current time

Assistant: Sure! Here's a light joke for you: Why don't scientists trust atoms? Because they make up everything.

Now, let me get the current time for you.

**Calling Tool: get_current_time**

The current time is 18:49:31. Enjoy your day!

You: